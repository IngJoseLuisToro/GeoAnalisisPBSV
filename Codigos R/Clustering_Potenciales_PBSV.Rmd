---
title: "Clustering Aplicado Para Analisis Pensiones Basicas Solidarias de Vejez PBSV"
subtitle: "Trabajo final Diplomado Big Data para politicas publicas"
author: "José Luis Toro"
date: "21 de Diciembre de 2018"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 4
    toc_float: yes
editor_options:
  chunk_output_type: inline
---

Verificamos directorio actual
```{r}
getwd()
```

Modificamos el directorio de trabajo correspondiente
```{r}
setwd("F:/archivo base Potenciales PBSV/data/FINAL")
```

Instalamos algunas librerias necesarias
```{r eval=FALSE}
#install.packages("lattice")
install.packages("ggplot2")
install.packages("caret")
install.packages("e1071")
install.packages("gmodels")
install.packages("dplyr")
install.packages("factoextra")
install.packages("NbClust")

library(lattice)
library(ggplot2)
library(caret)
library(e1071)
library(gmodels)
library(dplyr)
library(factoextra)
library(NbClust)
```

# Exploracion
###Cargamos los datos y realizamos un breve analisis de los datos.
```{r}
Potenciales = read.csv('Potenciales_PBSV_regionmetropol_v2.csv', dec=',')
str(Potenciales)
summary(Potenciales)
```
###Haremos una sustitucion de los valores NA en todo el dataset.
Se realiza la sustitución de los NA por el valor 0, pensando en que si no hay información asociada a estas variables, es debido a que sus niveles son muy malos y los paises no los quieren informar. Por lo tanto, poner un valor promedio en la sustitución, no entregaria una imagen real de lo que se quiere realizar a través del estudio.
```{r}
#acá le asigna el valor 0 a los NA
Potenciales[is.na(Potenciales)]<-0
```

#Clustering

##Realizaremos el Clustering del dataset utilizado

###Transformamos a valores numericos las variables a ingresar a la funcion kmeans.
```{r}
atributos <- Potenciales[8:15]

atributos_norm <- as.data.frame(lapply(atributos, scale))
```

###Ahora procedemos a buscar clusters:
```{r}
set.seed(0)
clusters <- kmeans(atributos_norm, 6)
```

###Vemos el tamano de cada cluster:
```{r}
clusters$size
```

###Procedemos a analizar cada cluster, a traves de los centroides
```{r}
clusters$centers
```

###Visualizamos los centroides
```{r}
View(clusters$centers)
```
###A partir de la observacion de los centroides podemos identificar  las variables mas llamativas de cada uno:
* 1: 
* 2: 
* 3: 
* 4:  
* 5:
* 6:


###Agregamos la columna del cluster al dataset original:
```{r}
Potenciales$cluster <- as.factor(clusters$cluster)
#View(Potenciales)
```
###Exportamos el dataset para realizar analisis visual:
```{r}
write.csv(Potenciales, file="clasif_potenciales.csv")
```


###Dados estos nuevos datos, podemos examinar como el cluster se relaciona con cada caracteristica.

Usando `aggregate` podemos examinar la demografia de cada cluster:
```{r}
#aggregate(data = Potenciales, Population ~ cluster, mean)
```

###Verificamos si la cantidad de cluster indicados es la mejor
```{r}
library(dplyr)
s=sample_n(atributos_norm, 100)  
```


```{r}
library(factoextra)
fviz_nbclust(s, kmeans, method = "wss", k.max=10)
```

```{r}
library(NbClust)
nb <- NbClust(s, distance = "euclidean", min.nc = 2,
        max.nc = 7, method = "kmeans")
fviz_nbclust(nb)
```
####Segun el analisis realizado el numero recomendado de cluster es de 6.

# Arbol de decision
###Entrenamiento
####Realizamos el entrenamiento del modelo en base al archivo de potenciales y los cluster definidos a traves del proceso k-means
```{r}
library(C50)
modelo = C5.0(atributos, Potenciales$cluster)
predicciones <- predict(modelo, atributos)
View(predicciones)
```
###Revisamos el modelo generado a traves de diversas formas visuales
```{r}
modelo
summary(modelo) #entrega el arbol como texto
plot(modelo)  #dibuja el modelo
plot(modelo, type="simple") #dibuja el modelo y nos da el numero de cluster
```

###CrossTable
####Realizamos el analisis del modelo generado, comparando el cluster generado por k-means y la prediccion realizada por el arbol de decision.
```{r}
library(gmodels)
CrossTable(x=Potenciales$cluster,y=predicciones,prop.chisq=FALSE)
```


###Matriz de confusion
####Continuamos analizando los clusters definidos y las predicciones de clusters del arbol de decision. 
```{r eval=FALSE}
install.packages("caret")
install.packages("lattice")
library(lattice)
library(caret)
confusionMatrix( predicciones, Potenciales$cluster)
```
###El modelo nos entrega una gran precision para la prediccion sobre los cluster.


#Cluster Jerarquico

Debido a que especificar el número de clústeres, y encontrar el número óptimo de clústeres resulta una tarea complicada. La agrupación jerárquica es un metodo alternativo que construye una jerarquia de abajo hacia arriba y no requiere que especifiquemos el número de clusteres de antemano.

El algoritmo funciona los dos cluster más cercanos y los combina en un solo grupo.

Generalmente se representa mediante una estructura de dendrograma.

Hay diferentes formas de determinar que tan cerca esten dos clústeres:

* Agrupación completa de enlaces: encuentra la distancia máxima posible entre los puntos que pertenecen a dos grupos diferentes.
* Agrupación de enlaces individuales: encuentra la distancia mínima posible entre los puntos que pertenecen a dos grupos diferentes.
* Agrupación media de enlaces: encuentra todas las distancias posibles en pares para los puntos que pertenecen a dos grupos diferentes y luego calcula el promedio.
* Agrupación de vinculación de centroide: encuentra el centroide de cada agrupacion y calcula la distancia entre los centroides de dos agrupaciones. 

```{r eval=FALSE}
clusters_jerarq <- hclust (dist (atributos))
plot (clusters_jerarq) 
```
```{r eval=FALSE}
clusterCut <- cutree (clusters_jerarq, 10) 
table (clusterCut, Potenciales$cluster)
```
###Utilizamos agrupación de vinculación de centroide: 
```{r eval=FALSE}
clusters_jerarq <- hclust (dist (atributos), method  = 'centroid')      
 plot (clusters_jerarq) 
```
```{r eval=FALSE}
library(ggplot2)
 ggplot (atributos, aes (Potenciales$cluster, Potenciales$Comuna_Fmt_google, color = Potenciales$cluster)) + 
   geom_point (alpha = 0.4, size = 3.5) + geom_point (col = clusterCut) + 
   scale_color_manual (values= c ('black', 'red', 'green','blue', 'pink', 'yellow','grey', 'purple', 'brown','orange')) 
```
```{r eval=FALSE}
hc <- hclust(dist(atributos), "cen")
memb <- cutree(hc, k = 10)
cent <- NULL
for(k in 1:10){
  cent <- rbind(cent, colMeans(atributos[memb == k, , drop = FALSE]))
}
hc1 <- hclust(dist(cent), method = "cen", members = table(memb))
opar <- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)
```


