---
title: 'Regresiones lineales en R'
subtitle: "[Diplomado en Big Data para Políticas Públicas](https://gobierno.uai.cl/diplomado/diplomado-big-data-politicas-publicas/)"
author: "José Luis Toro"
date: "20 de julio, 2018"
output:
  html_document: default
  html_notebook: default
---

En este ejemplo quisiera reforzar algunas ideas que ya han visto respecto a la estimación por *mínimos cuadrados ordinarios* (MCO o OLS) y profundizar (o presentar de otro modo) algunos conceptos. Todo lo anterior desde la idea de seguir aprendiendo en el uso del lenguajde/sofware `R`.

Para esto utilizaremos una base de datos con información (inventada) sobre sueldos y edad.

### Análisis descriptivo y correlaciones


```{r}
setwd("C:/Users/jtorop/Desktop/Diplomado Materias/BEstadisticas para analisis/R/Datos")
getwd()
sueldodf <- read.csv("Potenciales_PBSV_regionmetropol_v2.csv")
sueldodf <- sueldodf[8:16]
```

Con `attach()` lo que hacemos es ahorrarnos el tener que escribir `sueldodf$variable` cada vez que queramos trabajar con alguna de las variables de nuestro data set. Para revertir esto podemos usar `detach(sueldodf)`.

A continuación una vista a grandes rasgos de los datos:

```{r}
dim(sueldodf) #obtenemos la misma informacion usando nrow(sueldodf) y ncol(sueldodf)
names(sueldodf)
head(sueldodf) #también podrian usar tail(sueldodf)
```

Y como siempre, es bueno observar las estadisticas descriptivas de nuestras variables:

```{r}
summary(sueldodf)
```



```{r}
round(cor(x = sueldodf, method = "pearson"), 3)

modelo <- lm(PFP ~ EDAD + SEXO + FPS + YHL + YOI + YKL + AFPER2018, data = sueldodf )
summary(modelo)


round(cor(x = sueldodf, method = "pearson"), 3)

modelo2 <- lm(PFP ~ YOI, data = sueldodf )
summary(modelo2)

modelo3 <- lm(PFP ~ FPS, data = sueldodf )
summary(modelo3)
```





Ahora sabemos por ejemplo que dos variables son continuas y dos categóricas y que nuestra base de datos consta, por ejemplo, de tres carreras `Carrera1`, `Carrera2` o `Carrera3`.

Una función que puede ser útil en muchas ocasiones (al menos yo casi siempre lo uso cuando trabajo con bases de datos nuevas) es `pairs()`. Esta generará *scatter plots* de todas las variables vs todas las variables.

```{r, fig.align='center'}
pairs(sueldodf, col="blue")
install.packages("psych")
library(psych)
pairs.panels(sueldodf)
```

En este gráfico vemos, por ejemplo, que habría una relación entre edad y sueldo (lo cual no es tan sorprendente).

Profundicemos un poco en la relación de estas dos variables:

```{r, fig.align='center'}
plot(PFP, FPS, 
     xlab= "PFP", 
     ylab="FPS", 
     xlim=c(0,1206), 
     main="FPS vs. PFP")
```

El problema de concentrarse en solo dos variables es que perdemos información que podría ser valiosa. ¿Como incorporar más variables en este gráfico?

```{r, fig.align='center'}


plot(PFP, FPS, 
     xlab= "PFP", 
     ylab="FPS", 
     xlim=c(0,1206), 
     main="FPS vs. PFP")

text(PFP, FPS, 
     col=as.numeric(PBSV), 
     labels=PBSV, cex=.6)

legend("topleft", levels(PBSV), fil=c(1:2), cex=.6)
```

Ahora podemos ver que ademas de haber una clara relación entre sueldo y edad también pareciera que la gente con diplomado pareciera ganar más que aquella sin. ¿Encuentran alguna otra cosa gracias a este nuevo gráfico?

```{r}
table(PBSV, AFPER2018)
table(diplomado, carrera)
table(carrera, edad)
```

### Regresion lineal

Siempre es bueno comenzar con un análisis descriptivo de los datos pero, como ya saben, existen técnicas estadísticas mas formales que nos pueden ayudar a encontrar las relaciones existentes en nuestros datos y eventualmente evidencias de causalidad (no esta de mas parar acá para recordar que CORRELACION NO ES CAUSALIDAD).

Entonces, empecemos con un simple modelo lineal ($sueldo=\beta_{0}+\beta_{1}*edad+ \epsilon$) usando la estimación por *Mínimos Cuadrados Ordinarios*:

```{r}
fit = lm(sueldo~edad) #o glm(sueldo~edad, family="gaussian")
fit
```

El resultado de esta función es lo mismo que escribir: $sueldo=-2113389+85697*edad$

Noten que estos valores los podríamos calcular "a mano":

```{r}
covXY = sum((sueldo-mean(sueldo))*(edad-mean(edad)))/(nrow(sueldodf)-1) #covarianza de sueldo y tamano
varX = sum((edad-mean(edad))^2)/(nrow(sueldodf)-1)#Varianza de tamano
b1 = covXY/varX #coeficiente beta
b0 = mean(sueldo) - b1*mean(edad) #coeficiente del intercepto
c(b0, b1)
```

Esta regresión nos confirma la relación positiva entre edad y sueldo. En especifico nos dice que por cada incremento en una unidad de `edad` (años en este caso) hay un aumento (promedio) de $85.697 en sueldo.

Pero podríamos pedirle un poco mas de información a `fit`:

```{r}
summary(fit)
```

Acá vemos que la relación ya descrita ademas de ser positiva es "estadísticamente significativa" (p-value<0.001... ***). Y que `edad` explica 52% de la variabilidad en `sueldo`" (R cuadrado).

Nuevamente, algunos de estos valores los podemos calcular "a mano".. pero necesitamos un poco más de información:

```{r, fig.align="center", out.width = "1500px"}
knitr::include_graphics("6. Anova.PNG")
```

```{r}
anova(fit)
```

```{r}
RSS = sum((sueldo-fit$fitted.values)^2) # Suma de cuadrados residuales
ESS = sum((fit$fitted.values-mean(sueldo))^2) # Suma de cuadrados explicados
TSS = sum((sueldo-mean(sueldo))^2) #Suma de cuadrados totales (RSS+ESS)
#sum((sueldo-mean(sueldo))^2) == RSS+ESS

rsq = ESS/TSS # R cuadrado
rsq
sdb1 = sqrt((RSS/(nrow(sueldodf)-2))/(sum((edad-mean(edad))^2))) # desviacion estandar de beta1
sdb1
t = b1/sdb1 # estadistico t para beta 1
t
f = ((TSS-RSS)/1)/(RSS/(nrow(sueldodf)-1-1)) # estadistico f ((rsq/1)/(1-rsq)*(nrow(sueldodf)-2))
f 
```

Agreguemos esta nueva información (valores ajustados) a nuestro gráfico:

```{r, fig.align='center'}
plot(edad, sueldo, 
     xlab= "edad", 
     ylab="sueldo (pesos)", col=0, 
     xlim=c(30,66), 
     main="Figure 1: sueldo vs. edad")

text(edad, sueldo, 
     col=as.numeric(diplomado), 
     labels=carrera, 
     cex=.6)

legend("topleft", 
       levels(diplomado), 
       fil=c(1:2), 
       cex=.6)

abline(fit, col="blue", 
       lwd=2, lty=2) #nueva información

legend("bottomright", 
       "Fitted line", 
       lty=2, lwd=2, col="blue", cex=.6)
```

Pero, **¿es este un buen o un mal modelo?**

## Análizando nuestro modelo

Si utilizamos la función `plot()` con un objeto que contenga `lm()` nos arrojara una serie de "gráficos de diagnostico":

```{r, fig.align='center'}
par(mfrow=c(2,2))
plot(fit)
```

**¿¿Qué significa esto???**

Lo importante es recordar el ([teorema de Gauss-Markov](https://es.wikipedia.org/wiki/Teorema_de_Gauss-M%C3%A1rkov)) ("supuestos de Gauss Markov"). O como vieron en las clases sobre RLS/RLM, cumplir los siguientes supuestos:

  - Errores distribuidos normalmente
  - Errores con igual varianza (homocedastícidad)
  - Errores no correlacionados
  - No multicolinealidad

### ¿Es `fit` un buen o mal modelo?

```{r, fig.align='center'}
par(mfrow=c(1,2))

plot(fit$fitted.values, rstudent(fit), xlab="Valores Ajustados", ylab="Residuales Estudentizados", 
     main="Residuales Est. vs Valores Ajustados", col="blue")
abline(0,0, lty=2, col="red")

plot(fit$fitted.values, fit$residuals, xlab="Valores Ajustados", ylab="Residuales", 
     main="Residuales vs Valores Ajustados", col="blue")
abline(0,0, lty=2, col="red")

qqnorm(rstudent(fit), main="Q-Q Plot", col="blue")
abline(0,1, lty=2, col="blue")

hist(rstudent(fit), xlab="Residuales Estudentizados", 
     main="Histográma de Residuales", col="blue")
```

Respecto al primer supuesto, el Q-Q plot nos muestra que los residuales no están distribuidos normalmente, lo cual confirmamos al generar un histograma de los mismos residuales. También podemos hacer el ([Test de Shapiro-Wilk](https://es.wikipedia.org/wiki/Test_de_Shapiro%E2%80%93Wilk)) para probar lo mismo:

```{r}
shapiro.test(rstudent(fit))
shapiro.test(fit$residuals)
```

El *valor p* nos sugiere un rechazo de la hipótesis nula (normalidad de los datos).

Sobre el segundo supuesto podemos observar los primeros dos *plots* que nos indican que no se cumpliría el supuesto de igual varianza.

```{r, fig.align="center", out.width = "500px"}
knitr::include_graphics("6. homocedasticidad.PNG")
```

Para el tercer supuesto se puede mplementar una prueba estadística Durbin Watson:
```{r, cache=FALSE}
library(lmtest)
dwtest(fit)
```

El resultado (DW cercano a 2 y valor p) nos indica que no habría correlación entre los errores.

Finalmente, para el supuesto de no multicolinealidad

```{r, warning=TRUE, error=TRUE, cache=FALSE}
library(car)
vif(fit)
```

¿Por qué error?

Tratemos de experimentar con una transformación (logarítmica) de nuestra variable dependientea ver si podemos mejorar el modelo.

```{r}
sueldodf$lsueldo = log(sueldodf$sueldo)
attach(sueldodf)
#recuerden que también podría ser mutate(sueldodf, lsueldo = log(sueldo))
fit2 = lm(lsueldo~edad)
summary(fit2)
```

```{r, fig.align='center'}
par(mfrow=c(1,2))
plot(fit2$fitted.values, rstudent(fit2), xlab="Valores Ajustados", ylab="Residuales Estudentizados", 
     main="Residuales Est. vs Valores Ajustados", col="blue")
abline(0,0, lty=2, col="red")

plot(fit2$fitted.values, fit2$residuals, xlab="Valores Ajustados", ylab="Residuales", 
     main="Residuales vs Valores Ajustados", col="blue")
abline(0,0, lty=2, col="red")

qqnorm(rstudent(fit2), main="Q-Q Plot", col="blue")
abline(0,1, lty=2, col="blue")

hist(rstudent(fit2), xlab="Residuales Estudentizados", 
     main="Histográma de Residuales", col="blue")
```

```{r}
shapiro.test(rstudent(fit2))
shapiro.test(fit2$residuals)
dwtest(fit2)
```

Pareciera que nuestro escenario mejora cuando hacemos una transformación logarítmica de nuestra variable dependiente (`sueldo`).

```{r, fig.align='center'}
x = seq(32, 65, length=1000)
p = predict(fit2, newdata=data.frame(edad=x), interval="prediction")
y=exp(p)

par(mfrow=c(1,2))

plot(edad, lsueldo, xlab="edad", ylab="Log(sueldo)", 
     main="Log(sueldo) vs. edad")
abline(fit2, col="red")
legend("topleft", "OLS", lty=1, col="red", cex=.6)

plot(edad, sueldo, xlab="edad", ylab="sueldo", 
     main="sueldo vs. edad")
lines(x, y[,1], col="red")
abline(fit, col="blue", lty=2)

legend("topleft", c("Valores ajustados modelo log", "modelo original"), 
       lty=c(1,2), col=c("red", "blue"), cex=.6)
```

### Agregando una variable más

```{r}
fit3 = lm(lsueldo~edad+diplomado)
summary(fit3)
```

```{r, fig.align='center'}
par(mfrow=c(1,2))
plot(fit3$fitted.values, rstudent(fit3), xlab="Valores Ajustados", ylab="Residuales Estudentizados", 
     main="Residuales Est. vs Valores Ajustados", col="blue")
abline(0,0, lty=2, col="red")

plot(fit3$fitted.values, fit3$residuals, xlab="Valores Ajustados", ylab="Residuales", 
     main="Residuales vs Valores Ajustados", col="blue")
abline(0,0, lty=2, col="red")

qqnorm(rstudent(fit3), main="Q-Q Plot", col="blue")
abline(0,1, lty=2, col="blue")

hist(rstudent(fit3), xlab="Residuales Estudentizados", 
     main="Histográma de Residuales", col="blue")
```

```{r}
shapiro.test(rstudent(fit3))
shapiro.test(fit3$residuals)
dwtest(fit3)
```

## Otra forma de probar los supuestos

```{r, include=FALSE}
library(Hmisc)
library(ggplot2)
library(gridExtra)
```

Ejemplo con datos simulados:

```{r, fig.align='center', fig.width=13, warning=FALSE}
set.seed(7)

x = sort(rnorm(1000, 10, 100))[26:975]
y = x * 500 + rnorm(950, 5000, 20000)

df = data.frame(x = x, y = y, cuts = factor(cut2(x, g = 5)), resid = resid(lm(y ~ x)))

scatterPl = ggplot(df, aes(x = x, y = y)) + 
  geom_point(aes(colour = cuts, fill = cuts), shape = 1, show_guide = FALSE) + 
  geom_smooth(method = lm, level = 0.99)

plot_left = ggplot(df, aes(x = y, fill = cuts)) +
 geom_density(alpha = .5) + coord_flip() + scale_y_reverse()

plot_right = ggplot(data = df, aes(x = resid, fill = cuts)) +
geom_density(alpha = .5) + coord_flip()

grid.arrange(plot_left, scatterPl, plot_right, ncol=3, nrow=1, widths=c(1, 3, 1))
```

Y que significa esto?

```{r}
library(gvlma)
gvlma(lm(y ~ x))
```

### Y nuestros modelos?

```{r, fig.align='center', fig.width=13, warning=FALSE}
df = data.frame(x = edad, y = sueldo, cuts = factor(cut2(edad, g = 3)), resid = resid(fit))

scatterPl = ggplot(df, aes(x = x, y = y)) + 
  geom_point(aes(colour = cuts, fill = cuts), shape = 1, show_guide = FALSE) + 
  geom_smooth(method = lm, level = 0.99)

plot_left = ggplot(df, aes(x = y, fill = cuts)) + 
  geom_density(alpha = .5) + coord_flip() + scale_y_reverse()

plot_right = ggplot(data = df, aes(x = resid, fill = cuts)) + 
  geom_density(alpha = .5) + coord_flip()

grid.arrange(plot_left, scatterPl, plot_right, ncol=3, nrow=1, widths=c(1, 3, 1))
```

```{r}
gvlma(fit)
```

```{r, fig.align='center', fig.width=13, warning=FALSE}
df = data.frame(x = edad, y = lsueldo, cuts = factor(cut2(edad, g = 3)), resid = resid(fit2))

scatterPl = ggplot(df, aes(x = x, y = y)) + 
  geom_point(aes(colour = cuts, fill = cuts), shape = 1, show_guide = FALSE) + 
  geom_smooth(method = lm, level = 0.99)

plot_left = ggplot(df, aes(x = y, fill = cuts)) + 
  geom_density(alpha = .5) + coord_flip() + scale_y_reverse()

plot_right = ggplot(data = df, aes(x = resid, fill = cuts)) + 
  geom_density(alpha = .5) + coord_flip()

grid.arrange(plot_left, scatterPl, plot_right, ncol=3, nrow=1, widths=c(1, 3, 1))
```

```{r}
gvlma(fit2)
```

```{r, fig.align='center', fig.width=13, warning=FALSE}
df = data.frame(x = edad, y = lsueldo, cuts = factor(cut2(edad, g = 3)), resid = resid(fit3))

scatterPl = ggplot(df, aes(x = x, y = y)) + 
  geom_point(aes(colour = cuts, fill = cuts), shape = 1, show_guide = FALSE) + 
  geom_smooth(method = lm, level = 0.99)

plot_left = ggplot(df, aes(x = y, fill = cuts)) + 
  geom_density(alpha = .5) + coord_flip() + scale_y_reverse()

plot_right = ggplot(data = df, aes(x = resid, fill = cuts)) + 
  geom_density(alpha = .5) + coord_flip()

grid.arrange(plot_left, scatterPl, plot_right, ncol=3, nrow=1, widths=c(1, 3, 1))
```

```{r}
gvlma(fit3)
```